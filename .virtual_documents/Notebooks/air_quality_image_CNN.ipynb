import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import tensorflow as tf
from tensorflow.keras import layers, models, callbacks

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.utils.class_weight import compute_class_weight
from sklearn.metrics import classification_report



BASE_DIR = r"../datasets/Air Pollution Image Dataset"

all_dfs = []

for city in os.listdir(BASE_DIR):
    city_path = os.path.join(BASE_DIR, city)
    if not os.path.isdir(city_path):
        continue
        
    csv_path = os.path.join(city_path, f"{city}_AQI_ALL_info.csv")
    df = pd.read_csv(csv_path)
    df["city"] = city

    df["image_path"] = df.apply(
        lambda row: os.path.join(
            city_path,
            str(row["AQI_Class"]),
            row["Filename"]
        ),
        axis=1
    )

    all_dfs.append(df)

data = pd.concat(all_dfs, ignore_index=True)



# 1. Basic shape
print(data.shape)

# 2. Required columns check
print(data.columns)

# 3. Check broken image paths (VERY IMPORTANT)
broken = data[~data["image_path"].apply(os.path.exists)]
print("Broken images:", len(broken))

# 4. Class distribution
print(data["AQI_Class"].value_counts())

import matplotlib.pyplot as plt
data["AQI_Class"].value_counts().plot(kind="bar", title="AQI Class Distribution")
plt.show()



label_encoder = LabelEncoder()
data["AQI_Class_Encoded"] = label_encoder.fit_transform(data["AQI_Class"])

NUM_CLASSES = len(label_encoder.classes_)
print("Class mapping:")
for k, v in zip(label_encoder.classes_, range(NUM_CLASSES)):
    print(f"{k} -> {v}")



# Convert Hour column from 'HH:MM' â†’ HH (int)
data["Hour"] = data["Hour"].astype(str).str.split(":").str[0].astype(int)
print(data["Hour"].head())
print(data["Hour"].dtype)



TABULAR_FEATURES = [
    "Year", "Month", "Day", "Hour",
    "AQI",
    "PM2.5", "PM10",
    "O3", "CO", "SO2", "NO2"
]

X_tab = data[TABULAR_FEATURES]
y = data["AQI_Class_Encoded"].astype("int32")
img_paths = data["image_path"]



X_tab_train, X_tab_test, img_train, img_test, y_train, y_test = train_test_split(
    X_tab,
    img_paths,
    y,
    test_size=0.2,
    stratify=y,
    random_state=42
)



scaler = StandardScaler()
X_tab_train = scaler.fit_transform(X_tab_train)
X_tab_test = scaler.transform(X_tab_test)



img_train = img_train.tolist()
img_test = img_test.tolist()

y_train = y_train.values
y_test = y_test.values



IMG_SIZE = (224, 224)
BATCH_SIZE = 32

def load_image(path):
    path = tf.cast(path, tf.string)
    img = tf.io.read_file(path)
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.resize(img, IMG_SIZE)
    img = tf.cast(img, tf.float32) / 255.0
    return img

def create_dataset(img_paths, tab_data, labels, training=True):
    img_ds = tf.data.Dataset.from_tensor_slices(img_paths)
    img_ds = img_ds.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)

    tab_ds = tf.data.Dataset.from_tensor_slices(
        tf.cast(tab_data, tf.float32)
    )

    lbl_ds = tf.data.Dataset.from_tensor_slices(labels)

    ds = tf.data.Dataset.zip(((img_ds, tab_ds), lbl_ds))

    if training:
        ds = ds.shuffle(1024)

    return ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

train_ds = create_dataset(img_train, X_tab_train, y_train, training=True)
test_ds = create_dataset(img_test, X_tab_test, y_test, training=False)



class_weights = compute_class_weight(
    class_weight="balanced",
    classes=np.unique(y_train),
    y=y_train
)

class_weights = dict(enumerate(class_weights))
print("Class weights:", class_weights)



# Image branch
image_input = layers.Input(shape=(224, 224, 3))
base_model = tf.keras.applications.EfficientNetB0(
    include_top=False,
    weights="imagenet",
    input_tensor=image_input
)

base_model.trainable = False  # first phase

x = layers.GlobalAveragePooling2D()(base_model.output)
x = layers.BatchNormalization()(x)
x = layers.Dense(256, activation="relu")(x)
x = layers.Dropout(0.4)(x)

# Tabular branch
tab_input = layers.Input(shape=(X_tab_train.shape[1],))
t = layers.BatchNormalization()(tab_input)
t = layers.Dense(128, activation="relu")(t)
t = layers.Dropout(0.3)(t)
t = layers.Dense(64, activation="relu")(t)

# Fusion
combined = layers.concatenate([x, t])
combined = layers.Dense(256, activation="relu")(combined)
combined = layers.Dropout(0.5)(combined)

output = layers.Dense(NUM_CLASSES, activation="softmax")(combined)

model = models.Model(
    inputs=[image_input, tab_input],
    outputs=output
)



model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

model.summary()



cb = [
    callbacks.EarlyStopping(
        monitor="val_loss",
        patience=5,
        restore_best_weights=True
    ),
    callbacks.ReduceLROnPlateau(
        monitor="val_loss",
        factor=0.3,
        patience=3,
        min_lr=1e-6
    )
]



history = model.fit(
    train_ds,
    validation_data=test_ds,
    epochs=20,
    class_weight=class_weights,
    callbacks=cb
)



# Unfreeze top layers of EfficientNet
for layer in base_model.layers[-40:]:
    layer.trainable = True

model.compile(
    optimizer=tf.keras.optimizers.Adam(1e-5),
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

history_ft = model.fit(
    train_ds,
    validation_data=test_ds,
    epochs=10,
    class_weight=class_weights,
    callbacks=cb
)



y_pred = np.argmax(model.predict(test_ds), axis=1)

print(
    classification_report(
        label_encoder.inverse_transform(y_test),
        label_encoder.inverse_transform(y_pred)
    )
)




